# -*- coding: utf-8 -*-
"""CS4824_Project_namijain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yoaJ6agLDWhGEfKZj3sY00QSkgNA6Q6H
"""

# Commented out IPython magic to ensure Python compatibility.
# loading necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import pyplot
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, roc_auc_score, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from catboost import CatBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import accuracy_score,recall_score
from xgboost import XGBClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score, GridSearchCV

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# %config InlineBackend.figure_format = 'retina'

# to display all columns and rows:
pd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);

# reading the data
from google.colab import drive
drive.mount('/content/drive')

# reading the data
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/customer_churn.csv", index_col=0)

# The first 5 observation
df.head()

# The size of the data set
df.shape

# Feature information
df.info()

# Descriptive statistics of the data set
df.describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])

# categorical Variables
categorical_variables = [col for col in df.columns if col in "O"
                        or df[col].nunique() <=11
                        and col not in "Exited"]

categorical_variables

# Numeric Variables
numeric_variables = [col for col in df.columns if df[col].dtype != "object"
                        and df[col].nunique() >11
                        and col not in "CustomerId"]
numeric_variables

# Frequency of classes of dependent variable
df["Churn"].value_counts()

# Customers leaving the bank
churn = df.loc[df["Churn"]=="Yes"]

# Customers who did not leave the bank
not_churn = df.loc[df["Churn"]=="No"]

"""Categorical Values"""

# Frequency of not_churn group according to Tenure
not_churn["tenure"].value_counts().sort_values()

# Frequency of churn group according to Tenure
churn["tenure"].value_counts().sort_values()

"""Data Processing"""

# # Missing Observation Analysis
df.isnull().sum()

# To determine the threshold value for outliers
def outlier_thresholds(dataframe, variable, low_quantile=0.05, up_quantile=0.95):
    quantile_one = dataframe[variable].quantile(low_quantile)
    quantile_three = dataframe[variable].quantile(up_quantile)
    interquantile_range = quantile_three - quantile_one
    up_limit = quantile_three + 1.5 * interquantile_range
    low_limit = quantile_one - 1.5 * interquantile_range
    return low_limit, up_limit

# Are there any outliers in the variables
def has_outliers(dataframe, numeric_columns, plot=False):
   # variable_names = []
    for col in numeric_columns:
        low_limit, up_limit = outlier_thresholds(dataframe, col)
        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):
            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]
            print(col, " : ", number_of_outliers, "outliers")
            #variable_names.append(col)
            if plot:
                sns.boxplot(x=dataframe[col])
                plt.show()
    #return variable_names

# There is no outlier
for var in numeric_variables:
    print(var, "has " , has_outliers(df, [var]),  "Outliers")

"""Feature Engineering"""

# we standardize tenure with age
df["NewTenure"] = df["tenure"]/72
df["NewMonthylCharges"] = pd.qcut(df['MonthlyCharges'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df["NewTotalCharges"] = pd.qcut(df['TotalCharges'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])

df.head()

"""One Hot Encoding"""

# Variables to apply one hot encoding
list = ["gender", "MultipleLines"]
df = pd.get_dummies(df, columns =list, drop_first = True)

df.head()

print(df.columns)

"""Scaling"""

# Scale features using statistics that are robust to outliers.
def robust_scaler(variable):
    var_median = variable.median()
    quartile1 = variable.quantile(0.25)
    quartile3 = variable.quantile(0.75)
    interquantile_range = quartile3 - quartile1
    if int(interquantile_range) == 0:
        quartile1 = variable.quantile(0.05)
        quartile3 = variable.quantile(0.95)
        interquantile_range = quartile3 - quartile1
        if int(interquantile_range) == 0:
            quartile1 = variable.quantile(0.01)
            quartile3 = variable.quantile(0.99)
            interquantile_range = quartile3 - quartile1
            z = (variable - var_median) / interquantile_range
            return round(z, 3)

        z = (variable - var_median) / interquantile_range
        return round(z, 3)
    else:
        z = (variable - var_median) / interquantile_range
    return round(z, 3)

from sklearn.preprocessing import LabelEncoder, RobustScaler

# Identify columns that need scaling
new_cols_ohe = ["Gender_Male", "Multiple_Lines_Yes", "Multiple_Lines_No"]
like_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) <= 10]
cols_need_scale = [col for col in df.columns if col not in new_cols_ohe and col not in "Exited" and col not in like_num]

# Encode categorical columns before scaling
for col in df.columns:
    if df[col].dtype == 'O':  # Check if column is categorical
        if len(df[col].value_counts()) <= 10:  # Check if it has fewer than or equal to 10 unique values
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])

# Apply RobustScaler to numeric columns that need scaling
robust_scaler = RobustScaler()
for col in cols_need_scale:
    if df[col].dtype in ['int64', 'float64']:  # Make sure the column is numeric
        df[col] = robust_scaler.fit_transform(df[col].values.reshape(-1, 1))

df.head()

from sklearn.impute import SimpleImputer

# Create an imputer that replaces missing values with the median
imputer = SimpleImputer(strategy='median')

# Apply the imputer to the relevant columns
df[['TotalCharges', 'NewTotalCharges']] = imputer.fit_transform(df[['TotalCharges', 'NewTotalCharges']])

print(df.isnull().sum())

from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from lightgbm import LGBMClassifier

# Assuming df is already prepared

# Train-Test Separation
X = df.drop("Churn", axis=1)
y = df["Churn"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345)

# Models for Classification
models = [
    ('LR', LogisticRegression(random_state=123456)),
    ('KNN', KNeighborsClassifier()),
    ('CART', DecisionTreeClassifier(random_state=123456)),
    ('RF', RandomForestClassifier(random_state=123456)),
    ('SVR', SVC(gamma='auto', random_state=123456)),
    ('GB', GradientBoostingClassifier(random_state=12345)),
    ("LightGBM", LGBMClassifier(random_state=123456))
]

results = []
names = []

# Set shuffle=True to apply random_state
for name, model in models:
    kfold = KFold(n_splits=10, shuffle=True, random_state=123456)  # shuffle=True
    cv_results = cross_val_score(model, X, y, cv=kfold)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

""" GB Confusion Matrix"""

model_GB = GradientBoostingClassifier(random_state=12345)
model_GB.fit(X_train, y_train)
y_pred = model_GB.predict(X_test)
conf_mat = confusion_matrix(y_pred,y_test)
conf_mat

print("True Positive : ", conf_mat[1, 1])
print("True Negative : ", conf_mat[0, 0])
print("False Positive: ", conf_mat[0, 1])
print("False Negative: ", conf_mat[1, 0])

# Classification Report for XGB Model
print(classification_report(model_GB.predict(X_test),y_test))

# Auc Roc Curve
def generate_auc_roc_curve(clf, X_test):
    y_pred_proba = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    plt.plot(fpr,tpr,label="AUC ROC Curve with Area Under the curve ="+str(auc))
    plt.legend(loc=4)
    plt.show()
    pass

generate_auc_roc_curve(model_GB, X_test)

"""Model Tuning"""

# LightGBM:
lgb_model = LGBMClassifier()
# Model Tuning
lgbm_params = {'colsample_bytree': 0.5,
 'learning_rate': 0.01,
 'max_depth': 6,
 'n_estimators': 500}

lgbm_tuned = LGBMClassifier(**lgbm_params).fit(X, y)

#Let's choose the highest 4 models
# GBM
gbm_model = GradientBoostingClassifier()
# Model Tuning
gbm_params = {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1}
gbm_tuned = GradientBoostingClassifier(**gbm_params).fit(X,y)

# evaluate each model in turn
models = [("LightGBM", lgbm_tuned),
          ("GB",gbm_tuned)]
results = []
names = []
for name, model in models:
    kfold = KFold(n_splits=10, shuffle=True, random_state=123456)  # shuffle=True
    cv_results = cross_val_score(model, X, y, cv=kfold, scoring="accuracy")
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

for name, model in models:
        base = model.fit(X_train,y_train)
        y_pred = base.predict(X_test)
        acc_score = accuracy_score(y_test, y_pred)
        feature_imp = pd.Series(base.feature_importances_,
                        index=X.columns).sort_values(ascending=False)

        sns.barplot(x=feature_imp, y=feature_imp.index)
        plt.xlabel('Değişken Önem Skorları')
        plt.ylabel('Değişkenler')
        plt.title(name)
        plt.show()